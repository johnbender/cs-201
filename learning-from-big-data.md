# Prof. Tyson Condie - Learning from Big Data

Professor Condie began by outlining the high level reality around big data processing currently manifest in industry. Data is now cheaper and easier to get, so much so that it far surpasses Moore's Law. Every day with produce as much data as we did from the dawn of civilization until 2003.

This has created something of a gold rush to extract value from the enormous amounts of available data as evidenced by companies like Google and Facebook spending vast amounts of resources in attempting to leverage the information they gather to turn a profit. Otherwise the data available to most organizations is untapped because it is generally hard to deal with. That is it comes in less than ideal formats and is often partial. Moreover the questions that most users want to answer using data are exceptionally difficult to answer with any kind of surety and the tools used to answer those questions are clunky and ill suited to general purpose data manipulation.

Example questions that users often ask are: why us user engagement dropping? what is the difference between high tide traffic and a ddos attach? how should a given medical treatment be personalized? what ads should be shown to given user? Very generally, data is only as good as the questions that we ask and can answer with it.

Tyson continued by giving a broad overview of machine learning, characterizing it as programming by example. That is, it is frequently used when programming is hard, tedious, or when the program would need to change extremely frequently to address the problem at hand. He further split machine learning into two broad classes: supervised (classification, regression and recommendation) and unsupervised (clustering, dimensionality, and topic modeling). Further there are three parts to building a model, example formation, modeling and evaluation against a test set. He went on to consider an example in email classification for spam and to describe the process as iterative.

The most important part of the presentation covered the shortcomings of existing infrastructure to deal with problems that fall outside of the map-reduce model. Especially iterative machine learning algorithms that require multiple passes and are fairly inefficient. It is often the case that hadoop is used for its resource management and not it's map reduce capabilities and this forms a sort of "abuse" of the system. As a result there are many new systems that have been created by Google and other big data consumers, some of which he has worked on. His current work focuses on unifying the approaches in these various systems in to something of a "cloud OS".
