# Professor Heidi Alvarez, Open Science Data Cloud

Professor Heidi Alvarez presented on the open data cloud project which aims to provide scientists and researchers access to both data sets and the means to run long/complex computations over them. Her talk was mainly aimed at promoting the project and demonstrating how it might be useful for the attendees.

Professor Alvarez began by discussing some of the projects relevant statistics. It boasts nearly 6 petabytes of science based information and the typical job run with that or other data averages around a 1000 cores and around 10 terabytes of data. In addition they have 100 gigbit connections on semi-private backbone outside the usual internet backbone infrastructure to various campuses and research institutions around the world including UCLA from their center in Chicago, Illinois. Further the professor made light of some notable data sets including the largest cancer related database in the world and large amounts of earth science data from satellites. She also noted that they work closely with NASA and NIH on some of their data sets.

She went on to describe the particulars of the cloud's implementation. Primarily the jobs are Hadoop using open stack cloud infrastructure and opscode's chef for provisioning the servers. Other tools like R and nagios are also available for number crunching and server monitoring (respectively). Most of these details are abstracted away from the user but can be exposed down to the VM instance level where the details of charges per core hour were illustrated.
